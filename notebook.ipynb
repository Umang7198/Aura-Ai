{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d273428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Collecting data for Mumbai...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Aura.AI data collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Weather data collected for Mumbai\n",
      "INFO:__main__:News data collected for Mumbai: 34 articles\n",
      "INFO:__main__:Twitter data collected for Mumbai: 35 tweets\n",
      "INFO:__main__:Collecting data for Delhi...\n",
      "INFO:__main__:Weather data collected for Delhi\n",
      "INFO:__main__:News data collected for Delhi: 40 articles\n",
      "INFO:__main__:Twitter data collected for Delhi: 32 tweets\n",
      "INFO:__main__:Collecting data for Bangalore...\n",
      "INFO:__main__:Weather data collected for Bangalore\n",
      "INFO:__main__:News data collected for Bangalore: 27 articles\n",
      "INFO:__main__:Twitter data collected for Bangalore: 35 tweets\n",
      "INFO:__main__:Data saved to aura_data_20250813_234214.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AURA.AI DAILY MOOD REPORT ===\n",
      "Generated: 2025-08-13 23:42:14\n",
      "\n",
      "üèôÔ∏è MUMBAI\n",
      "   Mood: Very Positive (0.312)\n",
      "   Weather: Mist - 26.2¬∞C\n",
      "   Data Points: 34 news + 35 tweets\n",
      "   Confidence: 100%\n",
      "\n",
      "üèôÔ∏è DELHI\n",
      "   Mood: Positive (0.212)\n",
      "   Weather: Mist - 31.3¬∞C\n",
      "   Data Points: 40 news + 32 tweets\n",
      "   Confidence: 100%\n",
      "\n",
      "üèôÔ∏è BANGALORE\n",
      "   Mood: Positive (0.239)\n",
      "   Weather: Partly cloudy - 21.4¬∞C\n",
      "   Data Points: 27 news + 35 tweets\n",
      "   Confidence: 100%\n",
      "\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 69-70: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 335\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Save summary\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_summary_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 335\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 69-70: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AuraDataCollector:\n",
    "    def __init__(self):\n",
    "        # API Keys - Replace with your actual keys\n",
    "        self.news_api_key = \"47e7597b3e51433a81fcc3d908b1a792\"\n",
    "        self.weather_api_key = \"5a5c9571ba254b1c90b152042243103\"\n",
    "        self.twitter_api_key = \"984d78e1femsh2af122397b07360p15059bjsn900a6a3b90fa\"\n",
    "        \n",
    "        # Initialize sentiment analyzer\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Indian cities configuration\n",
    "        self.indian_cities = {\n",
    "            \"Mumbai\": {\"lat\": 19.0760, \"lon\": 72.8777, \"keywords\": [\"mumbai\", \"bombay\"]},\n",
    "            \"Delhi\": {\"lat\": 28.7041, \"lon\": 77.1025, \"keywords\": [\"delhi\", \"new delhi\"]},\n",
    "            \"Bangalore\": {\"lat\": 12.9716, \"lon\": 77.5946, \"keywords\": [\"bangalore\", \"bengaluru\"]},\n",
    "            \"Chennai\": {\"lat\": 13.0827, \"lon\": 80.2707, \"keywords\": [\"chennai\", \"madras\"]},\n",
    "            \"Kolkata\": {\"lat\": 22.5726, \"lon\": 88.3639, \"keywords\": [\"kolkata\", \"calcutta\"]},\n",
    "            \"Pune\": {\"lat\": 18.5204, \"lon\": 73.8567, \"keywords\": [\"pune\"]},\n",
    "            \"Hyderabad\": {\"lat\": 17.3850, \"lon\": 78.4867, \"keywords\": [\"hyderabad\"]},\n",
    "            \"Ahmedabad\": {\"lat\": 23.0225, \"lon\": 72.5714, \"keywords\": [\"ahmedabad\"]}\n",
    "        }\n",
    "        \n",
    "        # Initialize data storage\n",
    "        self.collected_data = []\n",
    "\n",
    "    def get_weather_data(self, city: str) -> Dict:\n",
    "        \"\"\"Fetch current weather data for a city\"\"\"\n",
    "        try:\n",
    "            city_info = self.indian_cities[city]\n",
    "            url = f\"http://api.weatherapi.com/v1/current.json\"\n",
    "            params = {\n",
    "                'key': self.weather_api_key,\n",
    "                'q': f\"{city_info['lat']},{city_info['lon']}\",\n",
    "                'aqi': 'yes'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract relevant weather info\n",
    "            weather_info = {\n",
    "                'city': city,\n",
    "                'temperature_c': data['current']['temp_c'],\n",
    "                'condition': data['current']['condition']['text'],\n",
    "                'humidity': data['current']['humidity'],\n",
    "                'wind_kph': data['current']['wind_kph'],\n",
    "                'air_quality_pm25': data['current'].get('air_quality', {}).get('pm2_5', 0),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Weather data collected for {city}\")\n",
    "            return weather_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching weather for {city}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def get_news_data(self, city: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news data related to a city\"\"\"\n",
    "        try:\n",
    "            news_items = []\n",
    "            city_keywords = self.indian_cities[city][\"keywords\"]\n",
    "            \n",
    "            # Search for city-specific news\n",
    "            for keyword in city_keywords:\n",
    "                url = \"https://newsapi.org/v2/everything\"\n",
    "                params = {\n",
    "                    'apiKey': self.news_api_key,\n",
    "                    'q': f\"{keyword} AND india\",\n",
    "                    'language': 'en',\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'pageSize': 20,\n",
    "                    'from': (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                for article in data.get('articles', []):\n",
    "                    # Analyze sentiment of headline and description\n",
    "                    text_to_analyze = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "                    sentiment_scores = self.analyzer.polarity_scores(text_to_analyze)\n",
    "                    \n",
    "                    news_item = {\n",
    "                        'city': city,\n",
    "                        'source': 'news',\n",
    "                        'title': article.get('title', ''),\n",
    "                        'description': article.get('description', ''),\n",
    "                        'url': article.get('url', ''),\n",
    "                        'published_at': article.get('publishedAt', ''),\n",
    "                        'sentiment_compound': sentiment_scores['compound'],\n",
    "                        'sentiment_positive': sentiment_scores['pos'],\n",
    "                        'sentiment_negative': sentiment_scores['neg'],\n",
    "                        'sentiment_neutral': sentiment_scores['neu'],\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                    news_items.append(news_item)\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(1)\n",
    "            \n",
    "            logger.info(f\"News data collected for {city}: {len(news_items)} articles\")\n",
    "            return news_items\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching news for {city}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_twitter_data(self, city: str) -> List[Dict]:\n",
    "        \"\"\"Fetch Twitter data for a city using RapidAPI\"\"\"\n",
    "        try:\n",
    "            tweets = []\n",
    "            city_keywords = self.indian_cities[city][\"keywords\"]\n",
    "            \n",
    "            # Search for tweets related to the city\n",
    "            for keyword in city_keywords:\n",
    "                url = \"https://twitter-api45.p.rapidapi.com/search.php\"\n",
    "                querystring = {\n",
    "                    \"query\": f\"{keyword} india\",\n",
    "                    \"search_type\": \"Top\"\n",
    "                }\n",
    "                \n",
    "                headers = {\n",
    "                    'x-rapidapi-key': self.twitter_api_key,\n",
    "                    'x-rapidapi-host': \"twitter-api45.p.rapidapi.com\"\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, headers=headers, params=querystring)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                # Process tweets (structure may vary based on API response)\n",
    "                if 'timeline' in data:\n",
    "                    for tweet in data['timeline'][:20]:  # Limit to 20 tweets per keyword\n",
    "                        tweet_text = tweet.get('text', '')\n",
    "                        \n",
    "                        # Clean tweet text\n",
    "                        cleaned_text = self.clean_text(tweet_text)\n",
    "                        \n",
    "                        # Analyze sentiment\n",
    "                        sentiment_scores = self.analyzer.polarity_scores(cleaned_text)\n",
    "                        \n",
    "                        tweet_item = {\n",
    "                            'city': city,\n",
    "                            'source': 'twitter',\n",
    "                            'text': cleaned_text,\n",
    "                            'user_followers': tweet.get('user', {}).get('followers_count', 0),\n",
    "                            'retweet_count': tweet.get('retweet_count', 0),\n",
    "                            'like_count': tweet.get('favorite_count', 0),\n",
    "                            'created_at': tweet.get('created_at', ''),\n",
    "                            'sentiment_compound': sentiment_scores['compound'],\n",
    "                            'sentiment_positive': sentiment_scores['pos'],\n",
    "                            'sentiment_negative': sentiment_scores['neg'],\n",
    "                            'sentiment_neutral': sentiment_scores['neu'],\n",
    "                            'timestamp': datetime.now().isoformat()\n",
    "                        }\n",
    "                        tweets.append(tweet_item)\n",
    "                \n",
    "                # Rate limiting for API calls\n",
    "                time.sleep(2)\n",
    "            \n",
    "            logger.info(f\"Twitter data collected for {city}: {len(tweets)} tweets\")\n",
    "            return tweets\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching Twitter data for {city}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text for sentiment analysis\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove user mentions and hashtags (keep the text after #)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def calculate_city_mood(self, city_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate overall mood metrics for a city\"\"\"\n",
    "        if not city_data:\n",
    "            return {}\n",
    "        \n",
    "        sentiments = [item['sentiment_compound'] for item in city_data if 'sentiment_compound' in item]\n",
    "        \n",
    "        if not sentiments:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate mood metrics\n",
    "        avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "        positive_ratio = len([s for s in sentiments if s > 0.1]) / len(sentiments)\n",
    "        negative_ratio = len([s for s in sentiments if s < -0.1]) / len(sentiments)\n",
    "        neutral_ratio = len([s for s in sentiments if -0.1 <= s <= 0.1]) / len(sentiments)\n",
    "        \n",
    "        # Determine overall mood\n",
    "        if avg_sentiment > 0.3:\n",
    "            mood = \"Very Positive\"\n",
    "        elif avg_sentiment > 0.1:\n",
    "            mood = \"Positive\" \n",
    "        elif avg_sentiment > -0.1:\n",
    "            mood = \"Neutral\"\n",
    "        elif avg_sentiment > -0.3:\n",
    "            mood = \"Negative\"\n",
    "        else:\n",
    "            mood = \"Very Negative\"\n",
    "        \n",
    "        return {\n",
    "            'avg_sentiment': round(avg_sentiment, 3),\n",
    "            'mood_label': mood,\n",
    "            'positive_ratio': round(positive_ratio, 3),\n",
    "            'negative_ratio': round(negative_ratio, 3),\n",
    "            'neutral_ratio': round(neutral_ratio, 3),\n",
    "            'total_items': len(city_data),\n",
    "            'confidence_score': min(len(city_data) / 50, 1.0)  # Confidence based on data volume\n",
    "        }\n",
    "\n",
    "    def collect_daily_data(self, cities: List[str] = None) -> Dict:\n",
    "        \"\"\"Collect data for specified cities for the current day\"\"\"\n",
    "        if cities is None:\n",
    "            cities = list(self.indian_cities.keys())[:5]  # Default to top 5 cities\n",
    "        \n",
    "        daily_data = {}\n",
    "        \n",
    "        for city in cities:\n",
    "            logger.info(f\"Collecting data for {city}...\")\n",
    "            \n",
    "            city_data = []\n",
    "            \n",
    "            # Collect weather data\n",
    "            weather_data = self.get_weather_data(city)\n",
    "            if weather_data:\n",
    "                city_data.append(weather_data)\n",
    "            \n",
    "            # Collect news data\n",
    "            news_data = self.get_news_data(city)\n",
    "            city_data.extend(news_data)\n",
    "            \n",
    "            # Collect Twitter data\n",
    "            twitter_data = self.get_twitter_data(city)\n",
    "            city_data.extend(twitter_data)\n",
    "            \n",
    "            # Calculate city mood\n",
    "            mood_metrics = self.calculate_city_mood([item for item in city_data if 'sentiment_compound' in item])\n",
    "            \n",
    "            daily_data[city] = {\n",
    "                'weather': weather_data,\n",
    "                'news_items': len(news_data),\n",
    "                'twitter_items': len(twitter_data),\n",
    "                'mood_metrics': mood_metrics,\n",
    "                'raw_data': city_data,\n",
    "                'collection_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Add delay between cities to respect rate limits\n",
    "            time.sleep(3)\n",
    "        \n",
    "        return daily_data\n",
    "\n",
    "    def save_data(self, data: Dict, filename: str = None):\n",
    "        \"\"\"Save collected data to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"aura_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Data saved to {filename}\")\n",
    "\n",
    "    def generate_summary_report(self, data: Dict) -> str:\n",
    "        \"\"\"Generate a human-readable summary of collected data\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=== AURA.AI DAILY MOOD REPORT ===\")\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        for city, city_data in data.items():\n",
    "            mood = city_data.get('mood_metrics', {})\n",
    "            weather = city_data.get('weather', {})\n",
    "            \n",
    "            report.append(f\"üèôÔ∏è {city.upper()}\")\n",
    "            report.append(f\"   Mood: {mood.get('mood_label', 'Unknown')} ({mood.get('avg_sentiment', 0):.3f})\")\n",
    "            report.append(f\"   Weather: {weather.get('condition', 'N/A')} - {weather.get('temperature_c', 'N/A')}¬∞C\")\n",
    "            report.append(f\"   Data Points: {city_data.get('news_items', 0)} news + {city_data.get('twitter_items', 0)} tweets\")\n",
    "            report.append(f\"   Confidence: {mood.get('confidence_score', 0):.0%}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize collector\n",
    "    collector = AuraDataCollector()\n",
    "    \n",
    "    # Collect data for top 3 cities (to start small)\n",
    "    cities_to_collect = [\"Mumbai\", \"Delhi\", \"Bangalore\"]\n",
    "    \n",
    "    print(\"Starting Aura.AI data collection...\")\n",
    "    daily_data = collector.collect_daily_data(cities_to_collect)\n",
    "    \n",
    "    # Save data\n",
    "    collector.save_data(daily_data)\n",
    "    \n",
    "    # Generate and print summary\n",
    "    summary = collector.generate_summary_report(daily_data)\n",
    "    print(summary)\n",
    "    \n",
    "    # Save summary\n",
    "    with open(f\"daily_summary_{datetime.now().strftime('%Y%m%d')}.txt\", 'w') as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8c734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 23:52:01,320 - INFO - üöÄ Starting data collection for 5 cities: Mumbai, Delhi, Bangalore, Chennai, Kolkata\n",
      "2025-09-04 23:52:01,321 - INFO - üìç [1/5] Collecting data for Mumbai...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Aura.AI Data Collection for Top 5 Indian Cities\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 23:52:01,655 - INFO - ‚úÖ Weather data collected for Mumbai: Mist, 27.2¬∞C\n",
      "2025-09-04 23:52:02,638 - ERROR - ‚ùå Network error fetching news for Mumbai: 429 Client Error: Too Many Requests for url: https://newsapi.org/v2/everything?apiKey=47e7597b3e51433a81fcc3d908b1a792&q=mumbai+AND+india&language=en&sortBy=publishedAt&pageSize=15&from=2025-09-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚èπÔ∏è  Data collection interrupted by user\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AuraDataCollector:\n",
    "    def __init__(self):\n",
    "        # API Keys - Replace with your actual keys\n",
    "        self.news_api_key = \"47e7597b3e51433a81fcc3d908b1a792\"\n",
    "        self.weather_api_key = \"5a5c9571ba254b1c90b152042243103\"\n",
    "        self.twitter_api_key = \"984d78e1femsh2af122397b07360p15059bjsn900a6a3b90fa\"\n",
    "        \n",
    "        # Initialize sentiment analyzer\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Top 5 Indian cities for focused data collection\n",
    "        self.indian_cities = {\n",
    "            \"Mumbai\": {\n",
    "                \"lat\": 19.0760, \"lon\": 72.8777, \n",
    "                \"keywords\": [\"mumbai\", \"bombay\", \"maharashtra\"],\n",
    "                \"population\": 12442373,\n",
    "                \"region\": \"Western India\"\n",
    "            },\n",
    "            \"Delhi\": {\n",
    "                \"lat\": 28.7041, \"lon\": 77.1025, \n",
    "                \"keywords\": [\"delhi\", \"new delhi\", \"ncr\"],\n",
    "                \"population\": 11007835,\n",
    "                \"region\": \"Northern India\"\n",
    "            },\n",
    "            \"Bangalore\": {\n",
    "                \"lat\": 12.9716, \"lon\": 77.5946, \n",
    "                \"keywords\": [\"bangalore\", \"bengaluru\", \"karnataka\"],\n",
    "                \"population\": 8443675,\n",
    "                \"region\": \"Southern India\"\n",
    "            },\n",
    "            \"Chennai\": {\n",
    "                \"lat\": 13.0827, \"lon\": 80.2707, \n",
    "                \"keywords\": [\"chennai\", \"madras\", \"tamil nadu\"],\n",
    "                \"population\": 4646732,\n",
    "                \"region\": \"Southern India\"\n",
    "            },\n",
    "            \"Kolkata\": {\n",
    "                \"lat\": 22.5726, \"lon\": 88.3639, \n",
    "                \"keywords\": [\"kolkata\", \"calcutta\", \"west bengal\"],\n",
    "                \"population\": 4496694,\n",
    "                \"region\": \"Eastern India\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize data storage\n",
    "        self.collected_data = []\n",
    "\n",
    "    def get_weather_data(self, city: str) -> Dict:\n",
    "        \"\"\"Fetch current weather data for a city\"\"\"\n",
    "        try:\n",
    "            city_info = self.indian_cities[city]\n",
    "            url = f\"http://api.weatherapi.com/v1/current.json\"\n",
    "            params = {\n",
    "                'key': self.weather_api_key,\n",
    "                'q': f\"{city_info['lat']},{city_info['lon']}\",\n",
    "                'aqi': 'yes'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract relevant weather info\n",
    "            weather_info = {\n",
    "                'city': city,\n",
    "                'temperature_c': data['current']['temp_c'],\n",
    "                'condition': data['current']['condition']['text'],\n",
    "                'humidity': data['current']['humidity'],\n",
    "                'wind_kph': data['current']['wind_kph'],\n",
    "                'feels_like_c': data['current']['feelslike_c'],\n",
    "                'uv_index': data['current']['uv'],\n",
    "                'air_quality_pm25': data['current'].get('air_quality', {}).get('pm2_5', 0),\n",
    "                'air_quality_pm10': data['current'].get('air_quality', {}).get('pm10', 0),\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_source': 'weather_api'\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"‚úÖ Weather data collected for {city}: {weather_info['condition']}, {weather_info['temperature_c']}¬∞C\")\n",
    "            return weather_info\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"‚ùå Network error fetching weather for {city}: {e}\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error fetching weather for {city}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def get_news_data(self, city: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news data related to a city\"\"\"\n",
    "        try:\n",
    "            news_items = []\n",
    "            city_keywords = self.indian_cities[city][\"keywords\"]\n",
    "            \n",
    "            # Search for city-specific news\n",
    "            for keyword in city_keywords:\n",
    "                url = \"https://newsapi.org/v2/everything\"\n",
    "                params = {\n",
    "                    'apiKey': self.news_api_key,\n",
    "                    'q': f\"{keyword} AND india\",\n",
    "                    'language': 'en',\n",
    "                    'sortBy': 'publishedAt',\n",
    "                    'pageSize': 15,  # Reduced to stay within limits\n",
    "                    'from': (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                for article in data.get('articles', []):\n",
    "                    # Skip articles with null/empty content\n",
    "                    title = article.get('title', '') or ''\n",
    "                    description = article.get('description', '') or ''\n",
    "                    \n",
    "                    if not title and not description:\n",
    "                        continue\n",
    "                    \n",
    "                    # Analyze sentiment of headline and description\n",
    "                    text_to_analyze = f\"{title} {description}\"\n",
    "                    sentiment_scores = self.analyzer.polarity_scores(text_to_analyze)\n",
    "                    \n",
    "                    news_item = {\n",
    "                        'city': city,\n",
    "                        'source': 'news',\n",
    "                        'title': title,\n",
    "                        'description': description,\n",
    "                        'url': article.get('url', ''),\n",
    "                        'source_name': article.get('source', {}).get('name', ''),\n",
    "                        'published_at': article.get('publishedAt', ''),\n",
    "                        'sentiment_compound': sentiment_scores['compound'],\n",
    "                        'sentiment_positive': sentiment_scores['pos'],\n",
    "                        'sentiment_negative': sentiment_scores['neg'],\n",
    "                        'sentiment_neutral': sentiment_scores['neu'],\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'data_source': 'news_api'\n",
    "                    }\n",
    "                    news_items.append(news_item)\n",
    "                \n",
    "                # Rate limiting - respect API limits\n",
    "                time.sleep(2)\n",
    "            \n",
    "            logger.info(f\"‚úÖ News data collected for {city}: {len(news_items)} articles\")\n",
    "            return news_items\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"‚ùå Network error fetching news for {city}: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error fetching news for {city}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_twitter_data(self, city: str) -> List[Dict]:\n",
    "        \"\"\"Fetch Twitter data for a city using RapidAPI\"\"\"\n",
    "        try:\n",
    "            tweets = []\n",
    "            city_keywords = self.indian_cities[city][\"keywords\"]\n",
    "            \n",
    "            # Search for tweets related to the city\n",
    "            for keyword in city_keywords[:2]:  # Limit to 2 keywords to reduce API calls\n",
    "                url = \"https://twitter-api45.p.rapidapi.com/search.php\"\n",
    "                querystring = {\n",
    "                    \"query\": f\"{keyword} india mood weather life\",\n",
    "                    \"search_type\": \"Top\"\n",
    "                }\n",
    "                \n",
    "                headers = {\n",
    "                    'x-rapidapi-key': self.twitter_api_key,\n",
    "                    'x-rapidapi-host': \"twitter-api45.p.rapidapi.com\"\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, headers=headers, params=querystring, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                # Process tweets (structure may vary based on API response)\n",
    "                if isinstance(data, dict) and 'timeline' in data:\n",
    "                    for tweet in data['timeline'][:10]:  # Limit to 10 tweets per keyword\n",
    "                        tweet_text = tweet.get('text', '') or ''\n",
    "                        \n",
    "                        if not tweet_text:\n",
    "                            continue\n",
    "                        \n",
    "                        # Clean tweet text\n",
    "                        cleaned_text = self.clean_text(tweet_text)\n",
    "                        \n",
    "                        if len(cleaned_text) < 10:  # Skip very short tweets\n",
    "                            continue\n",
    "                        \n",
    "                        # Analyze sentiment\n",
    "                        sentiment_scores = self.analyzer.polarity_scores(cleaned_text)\n",
    "                        \n",
    "                        tweet_item = {\n",
    "                            'city': city,\n",
    "                            'source': 'twitter',\n",
    "                            'text': cleaned_text,\n",
    "                            'user_followers': tweet.get('user', {}).get('followers_count', 0) if tweet.get('user') else 0,\n",
    "                            'retweet_count': tweet.get('retweet_count', 0),\n",
    "                            'like_count': tweet.get('favorite_count', 0),\n",
    "                            'created_at': tweet.get('created_at', ''),\n",
    "                            'sentiment_compound': sentiment_scores['compound'],\n",
    "                            'sentiment_positive': sentiment_scores['pos'],\n",
    "                            'sentiment_negative': sentiment_scores['neg'],\n",
    "                            'sentiment_neutral': sentiment_scores['neu'],\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                            'data_source': 'twitter_api'\n",
    "                        }\n",
    "                        tweets.append(tweet_item)\n",
    "                \n",
    "                # Rate limiting for API calls\n",
    "                time.sleep(3)\n",
    "            \n",
    "            logger.info(f\"‚úÖ Twitter data collected for {city}: {len(tweets)} tweets\")\n",
    "            return tweets\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"‚ùå Network error fetching Twitter data for {city}: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error fetching Twitter data for {city}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text for sentiment analysis\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove user mentions and hashtags (keep the text after #)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        \n",
    "        # Remove extra whitespace and special characters\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def calculate_city_mood(self, city_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate overall mood metrics for a city\"\"\"\n",
    "        if not city_data:\n",
    "            return {\n",
    "                'avg_sentiment': 0,\n",
    "                'mood_label': 'No Data',\n",
    "                'positive_ratio': 0,\n",
    "                'negative_ratio': 0,\n",
    "                'neutral_ratio': 0,\n",
    "                'total_items': 0,\n",
    "                'confidence_score': 0\n",
    "            }\n",
    "        \n",
    "        sentiments = [item['sentiment_compound'] for item in city_data if 'sentiment_compound' in item]\n",
    "        \n",
    "        if not sentiments:\n",
    "            return {\n",
    "                'avg_sentiment': 0,\n",
    "                'mood_label': 'No Sentiment Data',\n",
    "                'positive_ratio': 0,\n",
    "                'negative_ratio': 0,\n",
    "                'neutral_ratio': 0,\n",
    "                'total_items': len(city_data),\n",
    "                'confidence_score': 0\n",
    "            }\n",
    "        \n",
    "        # Calculate mood metrics\n",
    "        avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "        positive_ratio = len([s for s in sentiments if s > 0.1]) / len(sentiments)\n",
    "        negative_ratio = len([s for s in sentiments if s < -0.1]) / len(sentiments)\n",
    "        neutral_ratio = len([s for s in sentiments if -0.1 <= s <= 0.1]) / len(sentiments)\n",
    "        \n",
    "        # Determine overall mood\n",
    "        if avg_sentiment > 0.3:\n",
    "            mood = \"Very Positive\"\n",
    "        elif avg_sentiment > 0.1:\n",
    "            mood = \"Positive\" \n",
    "        elif avg_sentiment > -0.1:\n",
    "            mood = \"Neutral\"\n",
    "        elif avg_sentiment > -0.3:\n",
    "            mood = \"Negative\"\n",
    "        else:\n",
    "            mood = \"Very Negative\"\n",
    "        \n",
    "        return {\n",
    "            'avg_sentiment': round(avg_sentiment, 3),\n",
    "            'mood_label': mood,\n",
    "            'positive_ratio': round(positive_ratio, 3),\n",
    "            'negative_ratio': round(negative_ratio, 3),\n",
    "            'neutral_ratio': round(neutral_ratio, 3),\n",
    "            'total_items': len(city_data),\n",
    "            'confidence_score': round(min(len(sentiments) / 30, 1.0), 2)  # Confidence based on data volume\n",
    "        }\n",
    "\n",
    "    def collect_daily_data(self, cities: List[str] = None) -> Dict:\n",
    "        \"\"\"Collect data for all 5 major Indian cities\"\"\"\n",
    "        if cities is None:\n",
    "            cities = list(self.indian_cities.keys())  # All 5 cities\n",
    "        \n",
    "        daily_data = {}\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        logger.info(f\"üöÄ Starting data collection for {len(cities)} cities: {', '.join(cities)}\")\n",
    "        \n",
    "        for i, city in enumerate(cities, 1):\n",
    "            city_start_time = time.time()\n",
    "            logger.info(f\"üìç [{i}/{len(cities)}] Collecting data for {city}...\")\n",
    "            \n",
    "            city_data = []\n",
    "            \n",
    "            # Collect weather data\n",
    "            weather_data = self.get_weather_data(city)\n",
    "            if weather_data:\n",
    "                city_data.append(weather_data)\n",
    "            \n",
    "            # Collect news data\n",
    "            news_data = self.get_news_data(city)\n",
    "            city_data.extend(news_data)\n",
    "            \n",
    "            # Collect Twitter data\n",
    "            twitter_data = self.get_twitter_data(city)\n",
    "            city_data.extend(twitter_data)\n",
    "            \n",
    "            # Calculate city mood\n",
    "            mood_metrics = self.calculate_city_mood([item for item in city_data if 'sentiment_compound' in item])\n",
    "            \n",
    "            city_elapsed = time.time() - city_start_time\n",
    "            \n",
    "            daily_data[city] = {\n",
    "                'city_info': self.indian_cities[city],\n",
    "                'weather': weather_data,\n",
    "                'news_count': len(news_data),\n",
    "                'twitter_count': len(twitter_data),\n",
    "                'mood_metrics': mood_metrics,\n",
    "                'raw_data': city_data,\n",
    "                'collection_time_seconds': round(city_elapsed, 2),\n",
    "                'collection_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"‚úÖ {city} completed in {city_elapsed:.1f}s - Mood: {mood_metrics['mood_label']} ({mood_metrics['avg_sentiment']})\")\n",
    "            \n",
    "            # Add delay between cities to respect rate limits\n",
    "            if i < len(cities):  # Don't wait after the last city\n",
    "                logger.info(\"‚è≥ Waiting 5 seconds before next city...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        logger.info(f\"üéâ Data collection completed for all cities in {total_elapsed:.1f}s\")\n",
    "        \n",
    "        return daily_data\n",
    "\n",
    "    def save_data(self, data: Dict, filename: str = None):\n",
    "        \"\"\"Save collected data to JSON file with proper UTF-8 encoding\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"aura_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        try:\n",
    "            # Ensure directory exists\n",
    "            Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"üíæ Data saved to {filename}\")\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error saving data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_csv_summary(self, data: Dict, filename: str = None):\n",
    "        \"\"\"Save a CSV summary of the collected data\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"aura_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        \n",
    "        try:\n",
    "            summary_data = []\n",
    "            for city, city_data in data.items():\n",
    "                mood = city_data.get('mood_metrics', {})\n",
    "                weather = city_data.get('weather', {})\n",
    "                \n",
    "                summary_data.append({\n",
    "                    'city': city,\n",
    "                    'region': city_data.get('city_info', {}).get('region', ''),\n",
    "                    'mood_label': mood.get('mood_label', ''),\n",
    "                    'avg_sentiment': mood.get('avg_sentiment', 0),\n",
    "                    'positive_ratio': mood.get('positive_ratio', 0),\n",
    "                    'negative_ratio': mood.get('negative_ratio', 0),\n",
    "                    'temperature_c': weather.get('temperature_c', 0),\n",
    "                    'weather_condition': weather.get('condition', ''),\n",
    "                    'humidity': weather.get('humidity', 0),\n",
    "                    'air_quality_pm25': weather.get('air_quality_pm25', 0),\n",
    "                    'news_count': city_data.get('news_count', 0),\n",
    "                    'twitter_count': city_data.get('twitter_count', 0),\n",
    "                    'confidence_score': mood.get('confidence_score', 0),\n",
    "                    'collection_time': city_data.get('collection_timestamp', '')\n",
    "                })\n",
    "            \n",
    "            df = pd.DataFrame(summary_data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            logger.info(f\"üìä CSV summary saved to {filename}\")\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error saving CSV: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_summary_report(self, data: Dict) -> str:\n",
    "        \"\"\"Generate a human-readable summary of collected data\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\" * 50)\n",
    "        report.append(\"    AURA.AI DAILY MOOD REPORT - INDIA\")\n",
    "        report.append(\"=\" * 50)\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S IST')}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_news = sum(city_data.get('news_count', 0) for city_data in data.values())\n",
    "        total_tweets = sum(city_data.get('twitter_count', 0) for city_data in data.values())\n",
    "        \n",
    "        report.append(f\"üìä OVERALL STATISTICS:\")\n",
    "        report.append(f\"   Total Cities Analyzed: {len(data)}\")\n",
    "        report.append(f\"   Total News Articles: {total_news}\")\n",
    "        report.append(f\"   Total Tweets: {total_tweets}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # City-wise breakdown\n",
    "        report.append(\"üèôÔ∏è CITY-WISE MOOD BREAKDOWN:\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        for city, city_data in data.items():\n",
    "            mood = city_data.get('mood_metrics', {})\n",
    "            weather = city_data.get('weather', {})\n",
    "            \n",
    "            report.append(f\"üìç {city.upper()} ({city_data.get('city_info', {}).get('region', '')})\")\n",
    "            report.append(f\"   Mood: {mood.get('mood_label', 'Unknown')} (Score: {mood.get('avg_sentiment', 0):.3f})\")\n",
    "            report.append(f\"   Weather: {weather.get('condition', 'N/A')} - {weather.get('temperature_c', 'N/A')}¬∞C\")\n",
    "            report.append(f\"   Feels Like: {weather.get('feels_like_c', 'N/A')}¬∞C | Humidity: {weather.get('humidity', 'N/A')}%\")\n",
    "            report.append(f\"   Air Quality PM2.5: {weather.get('air_quality_pm25', 'N/A')}\")\n",
    "            report.append(f\"   Data: {city_data.get('news_count', 0)} news + {city_data.get('twitter_count', 0)} tweets\")\n",
    "            report.append(f\"   Confidence: {mood.get('confidence_score', 0):.0%}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Example usage and main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize collector\n",
    "    collector = AuraDataCollector()\n",
    "    \n",
    "    print(\"üöÄ Starting Aura.AI Data Collection for Top 5 Indian Cities\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Collect data for all 5 cities\n",
    "        daily_data = collector.collect_daily_data()\n",
    "        \n",
    "        # Save data in multiple formats\n",
    "        json_file = collector.save_data(daily_data)\n",
    "        csv_file = collector.save_csv_summary(daily_data)\n",
    "        \n",
    "        # Generate and print summary\n",
    "        summary = collector.generate_summary_report(daily_data)\n",
    "        print(\"\\n\" + summary)\n",
    "        \n",
    "        # Save summary with UTF-8 encoding\n",
    "        summary_file = f\"daily_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        try:\n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(summary)\n",
    "            print(f\"üìÑ Summary report saved to: {summary_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving summary: {e}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Collection complete! Files generated:\")\n",
    "        if json_file:\n",
    "            print(f\"   üìÅ JSON Data: {json_file}\")\n",
    "        if csv_file:\n",
    "            print(f\"   üìä CSV Summary: {csv_file}\")\n",
    "        print(f\"   üìÑ Text Summary: {summary_file}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è  Data collection interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during data collection: {e}\")\n",
    "        logger.error(f\"Fatal error: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30fe771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîë Testing API Key Validity\n",
      "==============================\n",
      "Status Code: 200\n",
      "‚úÖ API Key appears valid! Status: success\n",
      "Total Results: 19696\n",
      "üîç Debugging NewsData.io API Request\n",
      "==================================================\n",
      "API Key: pub_1e11c85ce5a947b881f961784d8f3ea0\n",
      "Request URL: https://newsdata.io/api/1/latest\n",
      "Params: {'apikey': 'pub_1e11c85ce5a947b881f961784d8f3ea0', 'q': 'india', 'size': 3}\n",
      "------------------------------\n",
      "üì° Making API request...\n",
      "Status Code: 200\n",
      "Error Response: {\n",
      "  \"status\": \"success\",\n",
      "  \"totalResults\": 21643,\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"article_id\": \"9c39b731ef93c1120cfb683d8ace039c\",\n",
      "      \"title\": \"VB arrests man on bribe charge\",\n",
      "      \"link\": \"https://timesofindia.indiatimes.com/city/ludhiana/vb-arrests-man-on-bribe-charge/articleshow/123705230.cms\",\n",
      "      \"keywords\": [\n",
      "        \"rajat sharma\",\n",
      "        \"arrest\",\n",
      "        \"punjab vigilance bureau\",\n",
      "        \"bribe charge\",\n",
      "        \"anti-corruption\"\n",
      "      ],\n",
      "      \"creator\": [\n",
      "        \"Text Size\"\n",
      "      ],\n",
      "      \"description\": null,\n",
      "      \"content\": \"ONLY AVAILABLE IN PAID PLANS\",\n",
      "      \"pubDate\": \"2025-09-04 18:27:56\",\n",
      "      \"pubDateTZ\": \"UTC\",\n",
      "      \"image_url\": \"https://static.toiimg.com/thumb/msid-117623938,imgsize-95379,width-400,height-225,resizemode-72/vb-arrests-man-on-bribe-charge.jpg\",\n",
      "      \"video_url\": null,\n",
      "      \"source_id\": \"toi\",\n",
      "      \"source_name\": \"The Times Of India\",\n",
      "      \"source_priority\": 2178,\n",
      "      \"source_url\": \"https://timesofindia.indiatimes.com\",\n",
      "      \"source_icon\": \"https://n.bytvi.com/toi.png\",\n",
      "      \"language\": \"english\",\n",
      "      \"country\": [\n",
      "        \"india\"\n",
      "      ],\n",
      "      \"category\": [\n",
      "        \"technology\"\n",
      "      ],\n",
      "      \"sentiment\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"sentiment_stats\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"ai_tag\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"ai_region\": \"ONLY AVAILABLE IN CORPORATE PLANS\",\n",
      "      \"ai_org\": \"ONLY AVAILABLE IN CORPORATE PLANS\",\n",
      "      \"ai_summary\": \"ONLY AVAILABLE IN PAID PLANS\",\n",
      "      \"ai_content\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"duplicate\": false\n",
      "    },\n",
      "    {\n",
      "      \"article_id\": \"2c765214e1ba737a2046a51aed89854e\",\n",
      "      \"title\": \"4 held in Buxar robbery case\",\n",
      "      \"link\": \"https://timesofindia.indiatimes.com/city/patna/4-held-in-buxar-robbery-case/articleshow/123705221.cms\",\n",
      "      \"keywords\": [\n",
      "        \"illegal firearms recovered\",\n",
      "        \"arms act violation\",\n",
      "        \"buxar robbery case\",\n",
      "        \"buxar sp shubham arya\",\n",
      "        \"sbi employee robbery\"\n",
      "      ],\n",
      "      \"creator\": [\n",
      "        \"Kamlesh Singh\"\n",
      "      ],\n",
      "      \"description\": \"Buxar police have arrested four individuals involved in the robbery of an SBI employee that occurred on July 8th. An SIT was formed, leading to the apprehension of three suspects attempting to sell stolen mobile phones.\",\n",
      "      \"content\": \"ONLY AVAILABLE IN PAID PLANS\",\n",
      "      \"pubDate\": \"2025-09-04 18:27:46\",\n",
      "      \"pubDateTZ\": \"UTC\",\n",
      "      \"image_url\": \"https://static.toiimg.com/thumb/msid-123705203,imgsize-38414,width-400,height-225,resizemode-72/image-for-representational-purpose.jpg\",\n",
      "      \"video_url\": null,\n",
      "      \"source_id\": \"toi\",\n",
      "      \"source_name\": \"The Times Of India\",\n",
      "      \"source_priority\": 2178,\n",
      "      \"source_url\": \"https://timesofindia.indiatimes.com\",\n",
      "      \"source_icon\": \"https://n.bytvi.com/toi.png\",\n",
      "      \"language\": \"english\",\n",
      "      \"country\": [\n",
      "        \"india\"\n",
      "      ],\n",
      "      \"category\": [\n",
      "        \"top\",\n",
      "        \"crime\"\n",
      "      ],\n",
      "      \"sentiment\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"sentiment_stats\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"ai_tag\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"ai_region\": \"ONLY AVAILABLE IN CORPORATE PLANS\",\n",
      "      \"ai_org\": \"ONLY AVAILABLE IN CORPORATE PLANS\",\n",
      "      \"ai_summary\": \"ONLY AVAILABLE IN PAID PLANS\",\n",
      "      \"ai_content\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"duplicate\": false\n",
      "    },\n",
      "    {\n",
      "      \"article_id\": \"9cad9b57e4ddcdd7de001e39a038ae9b\",\n",
      "      \"title\": \"Donald Trump Tariffs on India: \\u09b0\\u09be\\u09b6\\u09bf\\u09af\\u09bc\\u09be-\\u0987\\u0989\\u0995\\u09cd\\u09b0\\u09c7\\u09a8\\u09c7\\u09b0 \\u09af\\u09c1\\u09a6\\u09cd\\u09a7 \\u09a5\\u09be\\u09ae\\u09be\\u09a4\\u09c7\\u0987 \\u09ad\\u09be\\u09b0\\u09a4\\u0995\\u09c7 \\u09b6\\u09c1\\u09b2\\u09cd\\u0995-\\u0997\\u09c1\\u0981\\u09a4\\u09c7\\u09be \\u099f\\u09cd\\u09b0\\u09be\\u09ae\\u09cd\\u09aa\\u09c7\\u09b0...\\u099a\\u09be\\u099e\\u09cd\\u099a\\u09b2\\u09cd\\u09af\\u0995\\u09b0 \\u09a6\\u09be\\u09ac\\u09bf \\u09ae\\u09be\\u09b0\\u09cd\\u0995\\u09bf\\u09a8 \\u09b8\\u09c1\\u09aa\\u09cd\\u09b0\\u09bf\\u09ae \\u0995\\u09c7\\u09be\\u09b0\\u09cd\\u099f\\u09c7\",\n",
      "      \"link\": \"https://zeenews.india.com/bengali/world/trump-links-india-tariffs-to-ending-ukraine-war-in-supreme-court-plea_609319.html\",\n",
      "      \"keywords\": [\n",
      "        \"india\",\n",
      "        \"usa supreme court\",\n",
      "        \"donald trump\",\n",
      "        \"tariffs on india\",\n",
      "        \"supreme court plea usa\",\n",
      "        \"ukraine-russia\"\n",
      "      ],\n",
      "      \"creator\": [\n",
      "        \"Zee \\u09e8\\u09ea \\u0998\\u09a3\\u09cd\\u099f\\u09be\"\n",
      "      ],\n",
      "      \"description\": \"Trump on Tariffs: \\u09b0\\u09be\\u09af\\u09bc \\u0998\\u09c7\\u09be\\u09b7\\u09a3\\u09be\\u09b0 \\u09aa\\u09b0 \\u099f\\u09cd\\u09b0\\u09be\\u09ae\\u09cd\\u09aa \\u09b8\\u09c7\\u09be\\u09b6\\u09cd\\u09af\\u09be\\u09b2 \\u09ae\\u09bf\\u09a1\\u09bf\\u09af\\u09bc\\u09be \\u09aa\\u09cd\\u09b2\\u09cd\\u09af\\u09be\\u099f\\u09ab\\u09b0\\u09cd\\u09ae \\u099f\\u09cd\\u09b0\\u09c1\\u09a5 \\u09b8\\u09c7\\u09be\\u09b6\\u09cd\\u09af\\u09be\\u09b2-\\u098f \\u09b2\\u09bf\\u0996\\u09c7\\u099b\\u09c7\\u09a8, \\u201c\\u0986\\u09a6\\u09be\\u09b2\\u09a4 \\u09ad\\u09c1\\u09b2 \\u09ac\\u09b2\\u09c7\\u099b\\u09c7\\u0964 \\u09b6\\u09c1\\u09b2\\u09cd\\u0995 \\u09a4\\u09c1\\u09b2\\u09c7 \\u09a6\\u09c7\\u0993\\u09af\\u09bc\\u09be \\u0989\\u099a\\u09bf\\u09a4 \\u09a8\\u09af\\u09bc\\u0964 \\u09b6\\u09c7\\u09b7 \\u09aa\\u09b0\\u09cd\\u09af\\u09a8\\u09cd\\u09a4 \\u0986\\u09ae\\u09c7\\u09b0\\u09bf\\u0995\\u09be\\u0987 \\u099c\\u09bf\\u09a4\\u09ac\\u09c7\\u0964\\u201d \\u09a4\\u09bf\\u09a8\\u09bf \\u0986\\u09b0\\u0993 \\u099c\\u09be\\u09a8\\u09be\\u09a8, \\u09b8\\u09c1\\u09aa\\u09cd\\u09b0\\u09bf\\u09ae \\u0995\\u09c7\\u09be\\u09b0\\u09cd\\u099f\\u09c7 \\u0997\\u09bf\\u09af\\u09bc\\u09c7 \\u09b2\\u09a1\\u09bc\\u09be\\u0987 \\u099a\\u09be\\u09b2\\u09be\\u09ac\\u09c7\\u09a8\\u0964\",\n",
      "      \"content\": \"ONLY AVAILABLE IN PAID PLANS\",\n",
      "      \"pubDate\": \"2025-09-04 18:27:05\",\n",
      "      \"pubDateTZ\": \"UTC\",\n",
      "      \"image_url\": \"https://bengali.cdn.zeenews.com/bengali/sites/default/files/2025/09/04/564291-trump-cover.jpg\",\n",
      "      \"video_url\": null,\n",
      "      \"source_id\": \"zeenews\",\n",
      "      \"source_name\": \"Zee News\",\n",
      "      \"source_priority\": 2369,\n",
      "      \"source_url\": \"http://zeenews.india.com\",\n",
      "      \"source_icon\": \"https://n.bytvi.com/zeenews.png\",\n",
      "      \"language\": \"bengali\",\n",
      "      \"country\": [\n",
      "        \"india\"\n",
      "      ],\n",
      "      \"category\": [\n",
      "        \"world\"\n",
      "      ],\n",
      "      \"sentiment\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"sentiment_stats\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"ai_tag\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"ai_region\": \"ONLY AVAILABLE IN CORPORATE PLANS\",\n",
      "      \"ai_org\": \"ONLY AVAILABLE IN CORPORATE PLANS\",\n",
      "      \"ai_summary\": \"ONLY AVAILABLE IN PAID PLANS\",\n",
      "      \"ai_content\": \"ONLY AVAILABLE IN PROFESSIONAL AND CORPORATE PLANS\",\n",
      "      \"duplicate\": false\n",
      "    }\n",
      "  ],\n",
      "  \"nextPage\": \"1757010425032343038\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_newsdata_io_debug():\n",
    "    \"\"\"\n",
    "    Debug function to test NewsData.io API and identify the 422 error.\n",
    "    \"\"\"\n",
    "    # Your NewsData.io API Key \n",
    "    API_KEY = \"pub_1e11c85ce5a947b881f961784d8f3ea0\"  # Double-check this key!\n",
    "    \n",
    "    print(\"üîç Debugging NewsData.io API Request\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test with minimal parameters first\n",
    "    url = \"https://newsdata.io/api/1/latest\"\n",
    "    params = {\n",
    "        'apikey': API_KEY,\n",
    "        'q': 'india',  # Simple query first\n",
    "        'size': 3\n",
    "    }\n",
    "    \n",
    "    print(f\"API Key: {API_KEY}\")\n",
    "    print(f\"Request URL: {url}\")\n",
    "    print(f\"Params: {params}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    try:\n",
    "        print(\"üì° Making API request...\")\n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        \n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        \n",
    "        # Try to get the error details\n",
    "        try:\n",
    "            error_data = response.json()\n",
    "            print(f\"Error Response: {json.dumps(error_data, indent=2)}\")\n",
    "            \n",
    "            if 'message' in error_data:\n",
    "                print(f\"üî¥ Error Message: {error_data['message']}\")\n",
    "            if 'code' in error_data:\n",
    "                print(f\"üî¥ Error Code: {error_data['code']}\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Raw Response: {response.text}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Network error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "# Let's also test just the API key validity\n",
    "def test_api_key_only():\n",
    "    \"\"\"\n",
    "    Test if the API key is valid by making a simple request\n",
    "    \"\"\"\n",
    "    API_KEY = \"pub_1e11c85ce5a947b881f961784d8f3ea0\"\n",
    "    \n",
    "    url = \"https://newsdata.io/api/1/latest\"\n",
    "    params = {'apikey': API_KEY, 'q': 'test'}\n",
    "    \n",
    "    print(\"\\nüîë Testing API Key Validity\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"‚úÖ API Key appears valid! Status: {data.get('status')}\")\n",
    "            print(f\"Total Results: {data.get('totalResults', 0)}\")\n",
    "        else:\n",
    "            print(f\"‚ùå API Key may be invalid or expired\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing API key: {e}\")\n",
    "\n",
    "# Run both tests\n",
    "if __name__ == \"__main__\":\n",
    "    test_api_key_only()\n",
    "    test_newsdata_io_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee97756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing NewsData.io for Mumbai with keyword: 'mumbai'\n",
      "============================================================\n",
      "üì° Making API request...\n",
      "‚úÖ Found 5 articles for 'mumbai'\n",
      "============================================================\n",
      "\n",
      "üì∞ Article #1:\n",
      "   Title: ‚ÄòBigg Boss 19‚Äô: Housemates Plan Sweetest Surprise for Birthday Girl Neelam Giri, Contestants Dance Together on Bhojpuri Song (Watch Videos)\n",
      "   Source: latestly\n",
      "   Published: 2025-09-04 18:27:37\n",
      "   Sentiment: üëç POSITIVE (0.921)\n",
      "   Description: Big Boss contestants of the house planned a beautiful surprise for Neelam Giri and were seen dancing...\n",
      "   URL: https://www.latestly.com/entertainment/tv/bigg-boss-19-housemates-plan-sweetest-surprise-for-birthday-girl-neelam-giri-contestants-dance-together-on-bhojpuri-song-watch-videos-7094505.html\n",
      "--------------------------------------------------\n",
      "\n",
      "üì∞ Article #2:\n",
      "   Title: Entertainment News | Kartik Aaryan Announces Wrap of 'Tu Meri Main Tera Main Tera Tu Meri'\n",
      "   Source: latestly\n",
      "   Published: 2025-09-04 18:26:23\n",
      "   Sentiment: üëç POSITIVE (0.681)\n",
      "   Description: Get latest articles and stories on Entertainment at LatestLY. The shooting for Kartik Aaryan and Ana...\n",
      "   URL: https://www.latestly.com/agency-news/entertainment-news-kartik-aaryan-announces-wrap-of-tu-meri-main-tera-main-tera-tu-meri-7094544.html\n",
      "--------------------------------------------------\n",
      "\n",
      "üì∞ Article #3:\n",
      "   Title: S'pore-India partnership even more important in turbulent world, rooted in shared values: PM Wong\n",
      "   Source: mothership\n",
      "   Published: 2025-09-04 18:25:07\n",
      "   Sentiment: üëç POSITIVE (0.836)\n",
      "   Description: PM Wong and Modi also hugged again....\n",
      "   URL: https://mothership.sg/2025/09/spore-india-partnership-pm-wong/\n",
      "--------------------------------------------------\n",
      "\n",
      "üì∞ Article #4:\n",
      "   Title: Navi Mumbai Traffic Diversions Announced For Eid-e-Milad Procession On September 8; Check Out Details\n",
      "   Source: freepressjournal\n",
      "   Published: 2025-09-04 18:24:31\n",
      "   Sentiment: üòê NEUTRAL (0.000)\n",
      "   Description: According to officials, the procession will include around 8 to 10 tempos, 4 to 5 horse-drawn carria...\n",
      "   URL: https://www.freepressjournal.in/mumbai/navi-mumbai-traffic-diversions-announced-for-eid-e-milad-procession-on-september-8-check-out-details\n",
      "--------------------------------------------------\n",
      "\n",
      "üì∞ Article #5:\n",
      "   Title: Giorgio Armani passes away: Sonam Kapoor mourns the demise of Italian fashion designer\n",
      "   Source: toi\n",
      "   Published: 2025-09-04 18:21:12\n",
      "   Sentiment: üëç POSITIVE (0.052)\n",
      "   Description: The fashion world mourns the loss of Giorgio Armani, with Sonam Kapoor expressing her condolences. S...\n",
      "   URL: https://timesofindia.indiatimes.com/entertainment/hindi/bollywood/news/giorgio-armani-passes-away-sonam-kapoor-mourns-the-demise-of-italian-fashion-designer/articleshow/123704886.cms\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def test_newsdata_io_working():\n",
    "    \"\"\"\n",
    "    Working NewsData.io API test with sentiment analysis\n",
    "    \"\"\"\n",
    "    API_KEY = \"pub_1e11c85ce5a947b881f961784d8f3ea0\"  # Your working key\n",
    "    city = \"Mumbai\"\n",
    "    keyword = \"mumbai\"\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    print(f\"üß™ Testing NewsData.io for {city} with keyword: '{keyword}'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        url = \"https://newsdata.io/api/1/latest\"\n",
    "        params = {\n",
    "            'apikey': API_KEY,\n",
    "            'q': keyword,\n",
    "            'language': 'en',\n",
    "            'size': 5  # Get 5 articles\n",
    "        }\n",
    "\n",
    "        print(\"üì° Making API request...\")\n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('status') != 'success':\n",
    "            print(f\"‚ùå API Error: {data.get('message')}\")\n",
    "            return\n",
    "        \n",
    "        articles = data.get('results', [])\n",
    "        print(f\"‚úÖ Found {len(articles)} articles for '{keyword}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            title = article.get('title', 'No Title')\n",
    "            description = article.get('description', '') or ''\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            text_to_analyze = f\"{title} {description}\"\n",
    "            sentiment_scores = analyzer.polarity_scores(text_to_analyze)\n",
    "            \n",
    "            # Determine sentiment label\n",
    "            compound_score = sentiment_scores['compound']\n",
    "            if compound_score >= 0.05:\n",
    "                sentiment_label = \"üëç POSITIVE\"\n",
    "            elif compound_score <= -0.05:\n",
    "                sentiment_label = \"üëé NEGATIVE\"\n",
    "            else:\n",
    "                sentiment_label = \"üòê NEUTRAL\"\n",
    "            \n",
    "            print(f\"\\nüì∞ Article #{i}:\")\n",
    "            print(f\"   Title: {title}\")\n",
    "            print(f\"   Source: {article.get('source_id', 'Unknown')}\")\n",
    "            print(f\"   Published: {article.get('pubDate', 'Unknown')}\")\n",
    "            print(f\"   Sentiment: {sentiment_label} ({compound_score:.3f})\")\n",
    "            if description:\n",
    "                print(f\"   Description: {description[:100]}...\")\n",
    "            print(f\"   URL: {article.get('link', 'No URL')}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Network error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "# Run the working test\n",
    "if __name__ == \"__main__\":\n",
    "    test_newsdata_io_working()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
